{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    5100\n7    4732\n5    3009\n3    2696\n4    2496\n2    2420\n1    2284\n6    2263\nName: rating, dtype: int64"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(train, test, max_features, maxlen):\n",
    "    \"\"\"\n",
    "    Convert data to proper format.\n",
    "    1) Shuffle\n",
    "    2) Lowercase\n",
    "    3) Sentiments to Categorical\n",
    "    4) Tokenize and Fit\n",
    "    5) Convert to sequence (format accepted by the network)\n",
    "    6) Pad\n",
    "    7) Voila!\n",
    "    \"\"\"\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    X = train['text']\n",
    "    test_X = test['text']\n",
    "    Y = to_categorical(train.rating.values)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    return X, Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "maxlen = 125\n",
    "max_features = 10000\n",
    "\n",
    "X, Y, test_X = format_data(train, test, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[   0,    0,    0, ..., 6431,   11, 2789],\n       [1362,    7,   65, ...,  263,  169, 3611],\n       [   0,    0,    0, ..., 1741,  433,  950],\n       ...,\n       [2691, 1532, 1004, ...,  853,   79, 6584],\n       [   0,    0,    0, ...,   82, 7538, 1343],\n       [   0,    0,    0, ...,  197, 2151,   20]])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, BatchNormalization, Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input / Embdedding\n",
    "model.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "# CNN\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "# Output layer\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 18750 samples, validate on 6250 samples\nEpoch 1/5\n18750/18750 [==============================] - 25s 1ms/step - loss: 2.1373 - acc: 0.2257 - val_loss: 1.6894 - val_acc: 0.3550\nEpoch 2/5\n18750/18750 [==============================] - 24s 1ms/step - loss: 1.6556 - acc: 0.3697 - val_loss: 1.5856 - val_acc: 0.3859\nEpoch 3/5\n18750/18750 [==============================] - 25s 1ms/step - loss: 1.4673 - acc: 0.4374 - val_loss: 1.5758 - val_acc: 0.4000\nEpoch 4/5\n18750/18750 [==============================] - 25s 1ms/step - loss: 1.3112 - acc: 0.4953 - val_loss: 1.6579 - val_acc: 0.3867\nEpoch 5/5\n18750/18750 [==============================] - 25s 1ms/step - loss: 1.1521 - acc: 0.5583 - val_loss: 1.8724 - val_acc: 0.3635\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x256ca278>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 6203909164834603910\n]\n"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<contextlib._GeneratorContextManager at 0x174b68e30f0>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.device(\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit2b0a00106ba9480a82af446e93bacca9",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}